# LLM 原理深入解析

## 什麼是大型語言模型（LLM）

### 本質上就是「猜猜樂」和「接龍遊戲」

語言模型（Language Model）是一種統計模型，能根據「已經看到的文字」預測「下一個詞」，進而：

- 完成句子
- 填補缺詞
- 理解語義
- 甚至生成整段內容

核心目標是學會語言的結構與規律，**模擬人類的用語習慣**。

> 例如：
> 「我今天去 ____ 吃拉麵」
>  
> 模型會根據以往看過的句子，預測可能是「台北」、「新竹」、「學校附近」等地點。  
> 這種「填空猜字」的能力，就是語言模型的基礎邏輯。


---

## LLM 是如何學會語言？（訓練原理與推理方式）

### 版本一：原文版（技術視角）

#### 一、訓練方式

1. **自監督學習（Self-supervised Learning）**

   與需要人工標註的「監督學習」不同，LLM 採用的是**自監督學習**。  
   也就是說：模型會從輸入資料中**自己創造學習任務**，而不需要人類逐題標答案。

   常見例子：

   - **BERT：Masked Language Modeling（MLM）**
     - 做法：在句子中「遮住」某些詞，讓模型根據上下文把它們填回去。
     - 例：
       - 原句：`今天 天氣 很好 ，我想出去散步。`
       - 遮住後：`今天 天氣 [MASK] ，我想出去散步。`
       - 模型任務：根據前後文，預測 `[MASK]` 最可能是「很好」。

   - **GPT：Causal Language Modeling（CLM）**
     - 做法：從左到右，逐字／逐 token 預測**下一個詞**。
     - 例：
       - 已知：`今天 天氣 很好 ，我想 ……`
       - 任務：依序猜「出去」→「散」→「步」……

   透過這些任務，模型在沒有人工標記答案的情況下，依然可以**自動學習語言規則與語境邏輯**，大幅降低標註成本。

2. **損失函數與參數調整**

   在訓練過程中，每一次的預測結果都會與「正確答案」比對，產生一個稱為**損失值（Loss）**的數字，用來衡量預測的好壞。

   - 常見的損失函數：**交叉熵（Cross-entropy）**
     - 用來衡量「模型預測的機率分布」與「真實答案」之間的差異。

   模型會透過以下方法不斷修正自己：

   - **反向傳播（Backpropagation）**
   - **梯度下降（Gradient Descent）**

   在龐大的語料上，這個過程會被重複執行數十萬次甚至更多，  
   讓模型的參數逐步調整到「預測越來越準」，也就是我們說的「模型變得越來越聰明」。

---

#### 二、推理方式（Inference）

LLM 在推理的時候 **並不是在「理解」語言**，也沒有一個明確的「知識庫」。  
它做的事情比較像是：把語言的各種模式壓縮在**數百億甚至上千億個參數**裡，  
然後用這些參數來玩一場超大型的「機率選字遊戲」。

當使用者輸入一段文字時，LLM 大致會經過以下流程：

1. **語句編碼（Token Embedding）**  
   把文字切成 token，並轉換成向量表示。

2. **語境計算（Contextual Representation）**  
   使用 Transformer 結構計算各個位置之間的關聯，得到「在這個上下文下，每個位置應該長什麼樣子」的表示。

3. **機率預測（Next-token Prediction）**  
   模型對「下一個可能出現的 token」產生一個機率分布：  
   - 例如：「出去」20%、「回家」10%、「睡覺」5%……

4. **Softmax 選字與抽樣策略**  
   使用 Softmax 將分數轉成機率，再根據不同策略選出下一個 token：
   - 選機率最高的（argmax）
   - 或是用溫度、Top-k、Top-p 等抽樣方式，讓輸出更有多樣性。

> 重點：  
> LLM 做的是「機率選字」而不是「邏輯推理」或「事實查詢」。  
> 所以它有時會產生：
> - 語法上正確
> - 語義上看起來合理  
> 但**事實上錯誤**的內容。  
> 這種現象就叫做 **「幻覺（hallucination）」**，也是目前 LLM 的主要限制之一。

---

### 版本二：白話文版（比喻視角）

#### 一、訓練方式：先想像一個「超愛看書的機器人」

先把那些名詞（自監督、損失函數、梯度下降…）都丟掉，  
我們先想像有一個「超愛看書的機器人」。

---

##### 1. 以前的做法：老師辛苦出考卷（監督學習）

以前教機器人，大概長這樣：

- 老師先幫它做一大本題目本：
  - 題目：`今天 天氣 ____。`
  - 答案：`很好`
- 然後一題一題教：
  - 「這裡要填『很好』喔。」

這種方式的特色是：
- 每一題都要**人類先寫好題目和標準答案**
- 非常耗時、非常耗人力

這就是傳統的「監督學習」。

---

##### 2. 現在的做法：機器人自己把書改成考卷（自監督學習）

LLM 換了一種玩法：

- 不再需要老師一題一題出考卷
- 機器人直接拿到一大堆：
  - 小說
  - 文章
  - 對話紀錄
- 然後**自己**把這些內容「改造成題目」來練習。

常見有兩種遊戲規則：

---

###### A. BERT：貼貼紙玩「填空遊戲」

機器人拿到一句話：

> 「今天 天氣 很好 ，我想出去散步。」

它自己在某個詞上貼貼紙，把「很好」遮住：

> 「今天 天氣 ___ ，我想出去散步。」

然後問自己：

> 「根據前後文，空格最可能是什麼？」

這就是 **Masked Language Modeling（MLM）** 的概念：

> 👉 把句子裡某些詞遮住，  
> 👉 練習用上下文把空格填回去。

---

###### B. GPT：從左到右玩「接龍講故事」

換成 GPT 風格時，遊戲規則變成：

- 給它一句話的前半段：

  > 「今天 天氣 很好， 我想 ……」

- 它會一直問自己：

  > 「根據前面這些字，**下一個字**最可能是什麼？」

- 可能猜：
  - 「出去」→ 接著猜「散」→ 再猜「步」……

這就是 **Causal Language Modeling（CLM）**：

> 👉 從左到右，一個一個猜「下一個字是什麼」，  
> 👉 像是永遠沒完的故事接龍遊戲。

---

## Tokenization 是什麼？

---

### 一、認識 Token：語言模型處理的最小單位

在自然語言處理（NLP）中，**token** 是語言模型實際運算的最小語言單元。

- 它**不一定等於「一個字」**，
- 也**不完全等於「一個詞」**，
- 而是模型用來表示語義、進行推論與訓練的基礎單位。

例子：

> 中文句子：「我喜歡 AI」  
> 可能會被切成三個 token：`「我」`、`「喜歡」`、`「AI」`  

這些 token 接著會：

1. 先被轉成對應的 **索引（ID）**  
2. 再送入模型做 **編碼、理解與預測**

---

### 二、切分層級的差異與取捨

Tokenization 沒有唯一標準，會依據：

- 模型設計需求
- 語言特性

來決定「要切多細」。常見有三種粒度：

1. **詞級（Word level）**
2. **字符級（Character level）**
3. **子詞級（Subword level）**

---

#### 1. 詞級切分（Word level）

這是最直覺的方式：  
**把句子按照空白或語法邊界切成「詞」**。

- 例：英文 `Artificial Intelligence`  
  → 會被切成兩個 token：`Artificial`、`Intelligence`

**優點：**

- 符合人類直覺，每個 token 看起來都是「一個詞」

**缺點：**

- 詞彙表非常巨大（特別是多語言或專業領域）
- 對「沒見過的新詞」（out-of-vocabulary, OOV）几乎 **無能為力**

---

#### 2. 字符級切分（Character level）

字符級會把句子拆成**一個一個字元**：

- 英文 `user` → `u`、`s`、`e`、`r`
- 中文 `喜歡` → `喜`、`歡`

**優點：**

- 不會有 OOV 問題，因為所有字元一定在表內
- 詞彙表變得很小（只需要字母、標點、常用符號等）

**缺點：**

- 缺乏語義單位，「有意義的詞」被切得太碎
- 長單字／長詞會變得很長一串字元序列 →  
  **序列過長、運算成本高、語境不易建模**

---

#### 3. 子詞級切分（Subword level）

為了兼顧前面兩種方式的優缺點，  
**子詞級（Subword）切分逐漸成為主流**。

它的策略是：

- 常見詞：**盡量保留為一整個 token**
- 罕見詞：拆成幾個有意義的片段

例：

> `unbelievable` → `un`、`believe`、`able`

**好處是：**

- 高頻詞仍然是一個 token → 保留語義完整
- 低頻詞可以拆開 → 仍然能表示 & 處理新詞
- 在「語義表達能力」與「避免詞彙表爆炸」之間取得平衡

因此，**多數現代 LLM 都採用子詞級 Tokenization。**

---

### 三、BPE：從字元中產生子詞的有效方法

**Byte Pair Encoding（BPE）** 是實作子詞切分的代表方法之一。

大致流程：

1. 一開始只把每個「字元」當成 token
2. 統計語料中最常一起出現的「字元對」（pair）
3. 把這個字元對合併成一個新的 token
4. 重複步驟 2–3，持續合併，慢慢長出：
   - `e` + `r` → `er`
   - `un` + `believe` → `unbelieve`
   - …

這個過程會一直持續到：

- 達到預設的 **詞彙表大小**（例如 30k 或 50k 個子詞）

**優點：**

- 高頻片語或字串會被合併成單一 token（表達更有效率）
- 罕見字串可以拆成多個已知子詞（保有彈性）
- 即使遇到從未見過的新詞（技術名詞、新品牌），  
  仍可用既有子詞組合來表示，模型依然可以推論其語義。

---

### 四、Tokenization 對語言模型的實質影響

Tokenization 是**自然語言 → 模型可計算形式**之間的橋梁，選得好不好會直接影響：

- 語義理解能力
- 訓練與推理的效率
- 生成內容的流暢與品質

幾個關鍵影響：

1. **序列長度與運算成本**

   - 分得太細（例如純字元級）：
     - token 變多，序列變長
     - 計算量、記憶體消耗都會上升
   - 分得太粗：
     - 容易遇到大量 OOV，導致模型無法好好處理新詞或罕見詞

2. **訓練任務的訊號品質**

   - 以 **BERT** 為例：使用 Masked Language Modeling（MLM）
     - 會隨機遮蔽一部分 token，讓模型練習預測被遮住的內容
     - 如果 token 切得很差，要預測的單位就會變得模糊或過碎  
       → 影響模型學到的「上下文資訊」品質
   - **GPT 類模型**：
     - 每一步都在預測下一個 token
     - token 切得好不好，會直接影響：
       - 句子生成的流暢度
       - 模型對詞義與結構的掌握

---

### 五、中文場景下的特殊挑戰

子詞切分方法（如 BPE、WordPiece）在英文上很好用，  
但到了**中文**就遇到一些問題。

#### 1. 多語言 BPE 的「byte 級」問題

多語言版的 BPE 通常是針對 **Unicode 編碼的 byte 序列**做合併。

- 英文字母大多只佔 1 個 byte
- 中文一個字的 UTF-8 編碼常常需要 **3 個 byte**

結果變成：

- 一個漢字在最初會被拆成好幾個 byte token
- 若沒有額外優化 → 中文句子的 token 數量會暴增幾倍  
  → **同樣長度的中文，比英文更貴、更難算**

#### 2. 中文專用 Tokenizer 的出現

為了解決這些問題，許多中文相關的系統或模型會：

- 使用 **中文／多語言專用的 tokenization 工具**：
  - 如：SentencePiece、Jieba、改良版 BPE 等
- 或設計成：
  - **直接以「漢字」為 token**
  - 搭配針對中文語法特性調整過的子詞訓練策略

這樣可以：

- 降低中文相對於英文的運算負擔
- 更好地保留中文的語義和詞界資訊
- 提升模型在中文上的效率與準確度

---

## Tokenization 白話文版：把句子切成「積木」讓模型比較好用

下面這一段可以當簡報或教學用的「白話版說明」。

---

### 一、Token 是什麼？就像「積木塊」

可以把一句話想像成一串樂高積木：

- 人類看到的是：「我今天想吃拉麵」
- 模型看到的是：一塊一塊的 **token 積木**

例如模型可能把它拆成：

> `「我」｜「今天」｜「想」｜「吃」｜「拉麵」`

這些小積木（token）會被轉成數字 ID，  
才有辦法丟進模型裡做計算。

> ✅ 簡單記：  
> **Token = 模型眼中的小積木單位**，  
> 不是字也不一定是詞，而是「方便模型理解與計算的切法」。

---

### 二、不同切法的感覺：切太碎 or 切太粗？

想像你要教一個小孩讀句子，你可以有很多種切法：

#### 1. 詞級：一塊一塊是整個詞

像是：

> 「我｜今天｜想｜吃｜拉麵」

看起來最自然，也最像人類平常的「詞」。

**問題是：**

- 世界上的詞太多了（專有名詞、新品牌、新流行語…）
- 字典會大到炸掉
- 沒看過的新詞就完全不會認（OOV 問題）

---

#### 2. 字元級：切到每一個字、每一個字母

例如：

- 中文：「喜歡」→ `「喜」｜「歡」`
- 英文：「user」→ `u｜s｜e｜r`

**好處：**

- 再奇怪的詞也能拆開來處理
- 不會有「字典沒有收」的問題

**壞處：**

- 一句話會變成超長一串，像把樂高拆到一顆一顆
- 很難一眼看出「這一串是有意義的詞」
- 模型要花更多力氣去理解「這幾顆加在一起代表什麼」

---

#### 3. 子詞級：常見的整塊保留，不常見的拆一拆

這是現在最主流的作法，概念像：

> 常見的詞就讓它變成一塊大積木，  
> 罕見的詞拆成幾塊小積木。

例如英文：

- `unbelievable` → `un｜believe｜able`

**這樣的好處是：**

- 對常見詞（believe）保留完整意義
- 對新字或組合詞（例如某些新品牌名）仍然可以拆來表示
- 字典不會炸掉，模型也比較好懂句子在說什麼

---

### 三、BPE：模型自己從「字」學出比較大的積木

BPE 的感覺像是：

> 一開始，所有東西都只是一顆一顆小積木（字元）。  
> 模型看了一堆書之後，發現某些小積木常常黏在一起出現，  
> 就決定把它們黏成一塊比較大的「專用積木」。

例如模型會自己學到：

- `t` 跟 `h` 常常一起 → 變成 `th`
- `un` 跟 `believe` 常一起 → 變成 `unbelieve`
- 再從 `unbelieve` + `able` → `unbelievable`（視設定而定）

最後就得到一個「模型自己的積木字典」，裡面有：

- 單字元（a, b, c…）
- 片段（un, able…）
- 有時甚至是完整詞或詞的一部分

所以 BPE 在做的事是：

> **「從統計上常出現的字元組合，長出一套好用的積木系統」。**

---

### 四、為什麼 Token 切法會影響模型好不好用？

想像你在寫一段對話：

- 如果每個字都拆開，模型要看超長一串才能知道你在講什麼
- 如果每個新詞都不在字典裡，它根本看不懂你在說啥

Token 切得好，可以讓：

- 序列長度不會太長 → **運算省很多**
- 每個 token 盡量帶有語義 → **比較好學上下文**
- 預測下一個 token 時比較自然 → **生成結果比較順**

像：

- BERT 在玩的是「遮住一些 token 讓你猜」的遊戲  
  → token 切得不合理，遮起來就不好學
- GPT 在玩的是「下一個 token 是什麼」的遊戲  
  → token 切得好不好，直接影響它講話的流暢度和精準度

---

### 五、為什麼中文特別痛？

英文有空白、有詞界，比較好切；  
中文一整句貼在一起，沒有明顯分隔：

> 「我今天想吃拉麵」

如果拿「多語言通用、byte 級」的方法來切中文：

- 每個漢字會先被拆成幾個 byte
- 接著再拼回來 → 常常會變成：
  - 同一句中文，**token 數量比英文多很多**
  - 同樣長度的句子，中文要算的步驟比較多

所以：

- 很多中文模型會：
  - 直接用「一個漢字 = 一個 token」  
  - 或用專為中文設計／調整過的 tokenizer（SentencePiece、Jieba 等）
- 目標是讓：
  - 中文的 token 數量不要爆炸
  - 又能保留足夠語義資訊

> ✅ 白話總結：  
> **Tokenization = 把人類的句子切成模型看得懂的積木。**  
> 切太細 → 積木太多不好算；  
> 切太粗 → 模型學不會新詞。  
> 找到一個剛剛好的切法，就是現代 LLM 成功的關鍵之一。

---

## LLM 如何理解文字？

---

### 版本一：正式版（技術敘述）

---

### 一、電腦其實「不懂文字」——LLM 如何「理解」文字？

語言對人類來說是非常自然的表達方式，但對電腦而言卻是困難的挑戰。  
文字資料本質上是 **非結構化資料（Unstructured Data）**，不像表格或程式碼那樣具有明確欄位與格式，無法直接套用一般數值運算。

在自然語言處理（NLP）中，首要任務就是：

> 把「對人類直觀、對電腦混亂」的文字，  
> 轉成電腦能運算的結構化表示。

這樣的需求促成了 **語義向量（Embeddings）** 技術的發展。

在語言 AI 的運作流程中，一段輸入文字（非結構化）經過模型處理後，可以：

- 產生新的文字（生成回答、續寫）
- 給出分類結果（情感分析、意圖分類）
- 或轉換成 **語義向量（Embedding）**

這些向量可以用於：

- 問答系統（Question Answering）
- 推薦引擎（Recommendation）
- 檢索增強生成（Retrieval-Augmented Generation, RAG）
- 相似度搜尋、語義比對等下游任務

---

### 二、從 Token 到 Embedding

在 LLM 處理語言的第一步，並不是直接「理解意思」，  
而是先把句子拆解為一系列最小處理單位：**Token**。

- 這些 token 可能是：
  - 單字
  - 詞根
  - 子詞（Subword）
- 取決於所使用的斷詞／切詞演算法（Tokenization）

例子：

> 句子：`"My boss is a tiger"`  
> 可能被切成：`["my", "boss", "is", "a", "tiger"]`

完成切詞後，模型會為每個 token 配上一組 **數值向量**，稱為 **Embedding**。

- 這些向量是高維度數值陣列（例如 128 維、768 維…）
- 每一維可以看成某種「語義特徵」
- 每個詞或句子就像在一個 **高維語義空間** 裡的「座標點」

特性：

> 語義相近的詞，會被映射到彼此距離較近的位置。  
> 例如：`"dog"`、`"cat"`、`"puppy"` 會比 `"car"` 或 `"sky"` 更靠近彼此。

---

#### 傳統做法：Bag-of-Words（BoW）

Bag-of-Words 是最早期的「文字數值化」方法之一，其核心思想是：

> **不在乎詞語順序，只在乎哪些詞出現，以及出現幾次。**

流程大致如下：

1. 建立一份詞彙表（vocabulary）
2. 對每一句話統計：
   - 每個詞出現了幾次
3. 把這些統計結果組成向量（vector representation）

這種方法可以讓電腦：

- 接收固定長度的向量作為輸入
- 用於傳統機器學習（例如分類器）

但也有明顯缺點：

- 無法表示語序與語境
- 不懂詞與詞之間的深層語義關係

例如：

- `I love cats`
- `Cats love I`

在 Bag-of-Words 看起來幾乎是一樣的向量，  
因為它只關心「有哪些詞」與「出現頻率」，不關心誰愛誰。

這就是為什麼後來的語言模型（如 **Word2Vec、BERT、GPT**）  
必須引入 **contextual embedding（語境式向量）**，  
來捕捉上下文與詞語關係，使模型更接近「理解語義」。

---

### 三、Word2Vec：讓機器開始「猜」語義的模型

早期的方法（例如 Bag-of-Words），雖然能把文字轉成向量，  
但仍有關鍵限制：**無法真正捕捉語義與詞之間的關係。**

直到 2013 年，**Word2Vec** 的提出帶來重大突破：

- 透過淺層神經網路進行訓練
- 根據詞語在大量語料中的「共現模式」（哪些詞常一起出現）
- 學會詞與詞之間的語義關聯
- 形成早期效果良好的 **word embeddings** 技術

---

#### Word2Vec 的向量表示

在 Word2Vec 中：

- 每個詞會被映射為一組 **固定長度的向量**  
  （例如 100 維、300 維）
- 這些向量不是人為設計的標籤，而是：
  - 透過大量文本訓練
  - 根據「哪些詞常同時出現在類似語境中」自動學出來

舉例來說：

- `"model"` 常和 `"training"` 一起出現
- `"model"` 也經常和 `"AI"`、`"data"`、`"accuracy"` 同時出現

於是，模型會傾向把這些詞的向量放在語義空間中 **相近的位置**：

- `"model"`、`"algorithm"`、`"dataset"` → 彼此靠近
- `"banana"`、`"sunshine"` → 分佈在語義空間的另一區塊

---

#### 語義計算範例：向量也能做「類推題」

Word2Vec 的一大亮點，是：

> **向量之間可以做簡單運算，呈現語義關係。**

經典範例：

> `king - man + woman ≈ queen`

這代表：

- `"king"` 和 `"queen"` 的差異，大概對應到 `"man"` 和 `"woman"` 的差異
- 這種關係不是從字典抄來的，而是在大量語料中「自己學出來」的

本質上，embedding 的強大之處在於：

> 它不是用文字定義去描述詞意，  
> 而是用「詞與詞在相似語境中共同出現的統計結果」  
> 來讓模型慢慢學會「這些詞有點像」。

---

#### 語義盲點：無法處理「同一個詞多種意思」

儘管 Word2Vec 開啟了「語義運算」的新世界，但仍有重大限制：

> **每個詞不管出現在哪裡，只有一個固定向量。**

也就是說，它無法處理 **多義詞（polysemy）** 問題。

例子：

- `"Python"` 可以是：
  - 一種程式語言
  - 一種蛇
- `"bank"` 可以是：
  - 銀行
  - 河岸

在 Word2Vec 中：

- 遇到 `I'm coding in Python`
- 與 `The python slithered across the jungle`

這兩句中的 `"Python"` 都會被映射成 **同一個向量**，  
導致模型無法區分語義，降低語言理解的準確度。

這個限制也是後來 **BERT、GPT 等 contextual embedding 模型**  
被提出的重要原因之一：  
讓同一個詞在不同上下文中，可以有 **不同的向量表示**。

---

## 版本二：白話文＋舉例版

---

### 一、電腦真的「看得懂字」嗎？

先講白一點：

> 電腦其實看不到「中文」「英文」，  
> 它只看得到一長串 0 和 1。

如果我們只是把每個字變成編號，例如：

- `"貓"` → 101
- `"狗"` → 102
- `"愛"` → 103

電腦頂多知道：  
「喔，這句話有 101、102、103 這些號碼」，  
但它**完全不知道**：

- 101 跟 102 都是動物
- 103 跟「感情」有關

所以，我們需要一種方式，  
讓「意義相近的詞」在電腦眼裡也「站得比較近」。  
這就是 **Embedding** 在做的事。

---

### 二、從字變成「在語義地圖上的一個點」

你可以想像有一張看不到的「語義地圖」：

- 每個詞在上面是一個點
- 意思相近的點會靠得比較近

例如在這張地圖上：

- 「狗」、「貓」、「小狗」會站在同一區
- 「車子」、「公車」、「腳踏車」在另一區
- 「天空」、「宇宙」、「星星」又在另一區

這張地圖其實就是：

> 把每個詞變成一個 **高維度向量（很多維度的數字）**  
> 只是我們看不到那 100 維、300 維，在腦中可以簡化成「一張很大的地圖」。

---

### 三、Bag-of-Words：只數量、不管順序的古早味做法

以前的做法比較粗：

> 它只在乎「有哪些詞、出現幾次」，  
> 不在乎「出現的順序」。

所以在 Bag-of-Words 眼中：

- `I love cats`
- `Cats love I`

這兩句話的意思完全不一樣，  
但在它看來幾乎長一樣，因為都有：

- `I`、`love`、`cats`

這種方法雖然能讓電腦算東西，  
但對「真正的語義」幾乎是放棄狀態。

---

### 四、Word2Vec：讓電腦開始「看誰常跟誰一起出現」

Word2Vec 的想法比較聰明，可以想像成：

> 「我不懂你是什麼意思，  
>  但我會看你都跟誰混在一起。」

例如：

- 「model」常和「training」、「data」、「accuracy」一起出現
- 「banana」常和「fruit」、「yellow」、「sweet」一起出現

那在語義地圖上：

- 「model」會站得比較靠近「algorithm」、「dataset」
- 「banana」會站到另一群那邊去

於是就有了像這樣的「語義算術」：

> `king - man + woman ≈ queen`

感覺像：

- 把「king」扣掉「男性」的成分
- 再加上「女性」
- 差不多就會跑到「queen」那附近

這不是魔法，是因為：

> 電腦看了一大堆句子，  
> 把「誰常跟誰一起出現」統計到一種極致，  
> 然後用數字表示出來。

---

### 五、Word2Vec 的罩門：一個詞只有一個意思

雖然 Word2Vec 很酷，但有一個大問題：

> 「一個詞，不管放在哪句話，都只有一個向量。」

也就是說：

- `"Python"` 在程式語句裡是「程式語言」
- `"Python"` 在動物頻道裡是「蟒蛇」

但在 Word2Vec 看來：

> 不管是哪一種情況，  
> `"Python"` 都是同一個點，同一個向量。

同樣地：

- `"bank"` 是「銀行」還是「河岸」？  
  → 對它來說，也是一樣的向量。

這就會導致：

- 模型沒辦法真正理解多義詞
- 在需要精準語義的任務上，表現會有明顯限制

後來像 **BERT、GPT** 這種模型出現，  
就改成：

> 同一個詞，放在不同句子裡，可以有 **不同的向量表示**，  
> 也就是所謂的 **語境式向量（Contextual Embedding）**，  
> 讓「Python 是蛇還是程式語言」這件事，  
> 終於可以被上下文決定。

---